---
title: 'Different approaches to dimensionality reduction of DDM variables'
output:
github_document:
toc: yes
toc_float: yes
---

Motivation for this separate notebook: PCA does an ok job parsing out the different DDM variable types but EFA doesn't. Initially I thought theoretically EFA would be more approrpriate since I am trying to capture 'latent variables' and didn't want to assume that the variables don't have any unique variance. I was particularly reluctant to do this since the same task correlations for all variable types were higher than correlations across tasks.

My initial attempt to run EFA on EZ variables from test data gave some errors, the fit wasn't particularly good and the variable types did not really cluster separately. There was a cluster that included all the drift rates and another that included the majority of non-decision times. One could make an argument on using these clusters only as they capture separable processes but seems weak considering the bad fit of the model.

My earlier efforts (pre Neuroecon) used PCA. These did a better job in clustering the different variable types. This isn't too surprising given that PCA will assume all variability within a measure is due to common variance. One could build an argument suggesting that PCA could/should be used in this case because even if the clusters are not necessarily reflecting common cognitive processes they method serves the purpose of reducing the dimensionality of rich data. This too seems weak if it's not going to add anything to our understanding of the cognitive processes and will remain a primarily stastistical exercise.

More rencetly I am confused by other papers' use of PCA. E.g. the Econographics paper suggests:
> To summarize these clusters, we make use of principal components analysis (PCA), a statistical technique that produces components—linear combinations of variables—that explain as much variation in the underlying behaviors as possible, as discussed in Section 3. These components highlight latent dimensions underlying the econographic variables.

All of this is obviously very post-hoc. How should I go about this decision?

```{r}
library(tidyverse)
theme_set(theme_bw())
options(scipen = 1, digits = 4)

helper_func_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/'
source(paste0(helper_func_path, 'get_numeric_cols.R'))
source(paste0(helper_func_path, 'remove_outliers.R'))
source(paste0(helper_func_path, 'remove_correlated_task_variables.R'))
source(paste0(helper_func_path, 'transform_remove_skew.R'))
source(paste0(helper_func_path, 'get_demographics.R'))
source(paste0(helper_func_path, 'residualize_baseline.R'))
source(paste0(helper_func_path, 'find_optimal_components.R'))

ddm_workspace_scripts = '/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_DDM_Analyses/code/workspace_scripts/'
source(paste0(ddm_workspace_scripts,'ddm_measure_labels.R'))
source(paste0(ddm_workspace_scripts,'ddm_subject_data.R'))
rm(test_data_hddm_fullfit, test_data_hddm_refit)

fig_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_DDM_Analyses/output/figures/'
```

```{r}
clean_test_data = remove_correlated_task_variables(test_data)
```

Remove outliers (>2.5 SD away)

```{r}
clean_test_data = cbind(sub_id = clean_test_data$sub_id, as.data.frame(apply(clean_test_data[, -which(names(clean_test_data) %in% c("sub_id"))], 2, remove_outliers)))
```

Transform skewed variables

```{r}
numeric_cols = get_numeric_cols()
numeric_cols = numeric_cols[numeric_cols %in% names(clean_test_data) == T]
clean_test_data = transform_remove_skew(clean_test_data, numeric_cols)
```

Drop subject identifier column, mean impute and drop cols with no variance

```{r}
clean_test_data_std = clean_test_data %>% mutate_if(is.numeric, scale)
clean_test_data_std = clean_test_data_std %>% select(-sub_id)

#mean imputation
clean_test_data_std[is.na(clean_test_data_std)]=0

#drop cols with no variance
clean_test_data_std = clean_test_data_std %>%
  select_if(function(col) sd(col) != 0)
```

Extract EZ and HDDM variables

```{r}
clean_test_data_ez = clean_test_data_std %>%
  select(grep('EZ', names(clean_test_data_std), value=T))

clean_test_data_hddm = clean_test_data_std %>%
  select(grep('hddm', names(clean_test_data_std), value=T))
```

Residualize Age and Sex effects 

```{r}
data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/'
release = 'Complete_03-29-2018/'
dataset = 'demographic_health.csv'

demographics = get_demographics(dataset = paste0(data_path, release, dataset))

demographics = demographics %>%
  filter(X %in% clean_test_data$sub_id)
```

```{r}
clean_test_data_ez = cbind(clean_test_data_ez, demographics[,c("Age", "Sex")])

res_clean_test_data_ez = residualize_baseline(clean_test_data_ez)

res_clean_test_data_ez
```

## PCA on EZ variables of test data

```{r warning=FALSE, message=FALSE}
pca_ez_t1_comp_metrics = find_optimal_components(res_clean_test_data_ez, minc=2, maxc=20, model = "PCA")
```

```{r}
pca_ez_t1_comp_metrics
```

```{r}
ez_t1_pca_3 = principal(res_clean_test_data_ez, nfactors=3, rotate="oblimin")
```

```{r}
data.frame(ez_t1_pca_3$values) %>%
  rename(eig = ez_t1_pca_3.values) %>%
  arrange(-eig) %>%
  mutate(var_pct = eig/sum(eig)*100,
         pc = 1:n()) %>%
  filter(pc<11)%>%
  ggplot(aes(factor(pc), var_pct))+
  geom_bar(stat="identity")+
  ylab("Percentage of variance explained")+
  xlab("Principal component")
```

```{r}
data.frame(ez_t1_pca_3$values) %>%
  rename(eig = ez_t1_pca_3.values) %>%
  arrange(-eig) %>%
  mutate(var_pct = eig/sum(eig)*100,
         pc = 1:n(),
         var_pct_shift = lead(var_pct),
         var_pct_diff = var_pct - var_pct_shift) %>%
  filter(pc<11) %>%
  arrange(-var_pct_diff)
```

```{r}
ez_t1_pca_3_loadings = as.data.frame(ez_t1_pca_3$loadings[])

ez_t1_pca_3_loadings[abs(ez_t1_pca_3_loadings)<0.3]=NA

tmp = ez_t1_pca_3_loadings %>%
  mutate(dv = row.names(.)) %>%
  select(dv, TC1, TC2, TC3) %>%
  mutate(num_loading = 3-(is.na(TC1)+is.na(TC2)+is.na(TC3) ) ) %>%
  filter(num_loading!=0) %>%
  select(-num_loading) %>%
  arrange(-TC1, -TC2, -TC3) %>%
  mutate(order_num = 1:n(),
         dv = reorder(dv, -order_num)) %>%
  select(-order_num) %>%
  gather(Factor, Loading, -dv) %>%
  na.exclude() %>%
  mutate(load_sign = factor(ifelse(Loading>0,"pos","neg")))

var_type = ifelse(grepl("drift", tmp$dv), "#7fc97f",
                  ifelse(grepl("thresh", tmp$dv), "#beaed4",
                         ifelse(grepl("non_dec", tmp$dv), "#fdc086", NA)))         
tmp%>%         
  ggplot(aes(dv, abs(Loading), fill=load_sign))+
  geom_bar(stat = "identity")+
  facet_wrap(~Factor, nrow=1)+
  coord_flip()+
  xlab("")+
  theme(legend.position = "none",
        axis.text.y = element_text(color = var_type))+
  ylab("Absolute Loading")
```

## EFA on EZ variables of test data

```{r warning=FALSE, message=FALSE}
efa_ez_t1_comp_metrics = find_optimal_components(res_clean_test_data_ez, fm = "minres", minc=2)
```

```{r}
efa_ez_t1_comp_metrics
```

```{r}
ez_t1_fa_8 = fa(res_clean_test_data_ez, efa_ez_t1_comp_metrics$comp[1], rotate='oblimin', fm='minres', scores='tenBerge')
```

```{r}
ez_t1_fa_3 = fa(res_clean_test_data_ez, 3, rotate='oblimin', fm='minres', scores='tenBerge')
```

```{r}
anova(ez_t1_fa_3, ez_t1_fa_8)
```

```{r}
ez_t1_fa_3_loadings = as.data.frame(ez_t1_fa_3$loadings[])

ez_t1_fa_3_loadings[abs(ez_t1_fa_3_loadings)<0.3]=NA

tmp = ez_t1_fa_3_loadings %>%
  mutate(dv = row.names(.)) %>%
  select(dv, MR1, MR2, MR3) %>%
  mutate(num_loading = 3-(is.na(MR1)+is.na(MR2)+is.na(MR3) ) ) %>%
  filter(num_loading!=0) %>%
  select(-num_loading) %>%
  arrange(-MR1, -MR2, -MR3) %>%
  mutate(order_num = 1:n(),
         dv = reorder(dv, -order_num)) %>%
  select(-order_num) %>%
  gather(Factor, Loading, -dv) %>%
  na.exclude() %>%
  mutate(load_sign = factor(ifelse(Loading>0,"pos","neg")))

var_type = ifelse(grepl("drift", tmp$dv), "#7fc97f",
                  ifelse(grepl("thresh", tmp$dv), "#beaed4",
                         ifelse(grepl("non_dec", tmp$dv), "#fdc086", NA)))         
tmp%>%         
  ggplot(aes(dv, abs(Loading), fill=load_sign))+
  geom_bar(stat = "identity")+
  facet_wrap(~Factor, nrow=1)+
  coord_flip()+
  xlab("")+
  theme(legend.position = "none",
        axis.text.y = element_text(color = var_type))+
  ylab("Absolute Loading")
```

```{r}
ez_t1_fa_8_loadings = as.data.frame(ez_t1_fa_8$loadings[])

ez_t1_fa_8_loadings[abs(ez_t1_fa_8_loadings)<0.3]=NA

tmp = ez_t1_fa_8_loadings %>%
  mutate(dv = row.names(.)) %>%
  select(dv, MR1, MR2, MR3, MR4, MR5, MR6, MR7, MR8) %>%
  mutate(num_loading = 8-(is.na(MR1)+is.na(MR2)+is.na(MR3)+is.na(MR4)+is.na(MR5)+is.na(MR6)+is.na(MR7)+is.na(MR8) ) ) %>%
  filter(num_loading!=0) %>%
  select(-num_loading) %>%
  arrange(-MR1, -MR2, -MR3,-MR1, -MR5, -MR6,-MR7, -MR8) %>%
  mutate(order_num = 1:n(),
         dv = reorder(dv, -order_num)) %>%
  select(-order_num) %>%
  gather(Factor, Loading, -dv) %>%
  na.exclude() %>%
  mutate(load_sign = factor(ifelse(Loading>0,"pos","neg")))

var_type = ifelse(grepl("drift", tmp$dv), "#7fc97f",
                  ifelse(grepl("thresh", tmp$dv), "#beaed4",
                         ifelse(grepl("non_dec", tmp$dv), "#fdc086", NA)))         
tmp%>%         
  ggplot(aes(dv, abs(Loading), fill=load_sign))+
  geom_bar(stat = "identity")+
  facet_wrap(~Factor, nrow=1)+
  coord_flip()+
  xlab("")+
  theme(legend.position = "none",
        axis.text.y = element_text(color = var_type))+
  ylab("Absolute Loading")
```


## PCA on HDDM variables of test data

## EFA on HDDM variables of test data

## PCA on EZ variables of retest data

## EFA on EZ variables of retest data

## PCA on HDDM variables of retest data

## EFA on HDDM variables of retest data