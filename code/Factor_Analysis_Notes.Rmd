---
title: 'Factor Analysis Notes'
output:
github_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_DDM_Analyses/code/SRO_DDM_Analyses_Helper_Functions.R')

test_data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_03-29-2018/'

retest_data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-29-2018/'

input_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_DDM_Analyses/input/'

library(FactoMineR)
library(missForest)
library(factoextra)
```


```{r}
measure_labels <- read.csv(paste0(input_path, 'measure_labels.csv'))

measure_labels = measure_labels %>% 
  select(-measure_description) %>%
  filter(ddm_task == 1) %>%
  select(-ddm_task) %>%
  filter(rt_acc != "other") %>%
  mutate(dv = as.character(dv),
         overall_difference = factor(overall_difference,levels = c("overall", "difference", "condition"), labels = c("non-contrast", "contrast", "condition")),
         ddm_raw = ifelse(raw_fit == "raw", "raw", "ddm")) %>%
  separate(dv, c("task_group", "var"), sep = "\\.", remove=FALSE, extra = "merge")
```

### Load time 1 data
```{r}
test_data <- read.csv(paste0(test_data_path,'variables_exhaustive.csv'))

test_data_subs = as.character(test_data$X)

test_data = test_data %>%
  select(measure_labels$dv)

test_data$sub_id = test_data_subs

rm(test_data_subs)
```

### Load time 2 data 
```{r}
retest_data <- read.csv(paste0(retest_data_path,'variables_exhaustive.csv'))

retest_data_subs = as.character(retest_data$X)

retest_data = retest_data %>%
  select(measure_labels$dv)

retest_data$sub_id = retest_data_subs

retest_data = retest_data[retest_data$sub_id %in% test_data$sub_id,]

rm(retest_data_subs)
```

Standardize data and (mean) impute

```{r}
#Standardize datasets
test_data_std = test_data %>% mutate_if(is.numeric, scale)
test_data_std = test_data_std %>% select(-sub_id)

retest_data_std = retest_data %>% mutate_if(is.numeric, scale)
retest_data_std = retest_data_std %>% select(-sub_id)

#mean imputation on standardized data
test_data_std[is.na(test_data_std)]=0
retest_data_std[is.na(retest_data_std)]=0
```

#Difference between PCA and EFA

>Despite all these similarities, there is a fundamental difference between them: PCA is a linear combination of variables; Factor Analysis is a measurement model of a latent variable.

[Source](https://www.theanalysisfactor.com/the-fundamental-difference-between-principal-component-analysis-and-factor-analysis/)

#PCA

## `FactoMineR` package

Following the tutorial [here](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/)

```{r eval=FALSE}
#Imputation with missForest package - SLOW
test_data_std_miss <- missForest(as.matrix(test_data_std))
```

```{r}
test_pca <- PCA(test_data_std, graph = FALSE, scale.unit = FALSE)
```

```{r}
test_pca
```

Eigenvalues don't go <1 until dimension 89. 

NOTE: Normally you should have as many eigenvalues as variables (columns) BUT if you fit it to data with more variables than individuals (row) you only have `nrow` eigenvalues.

```{r}
eig_val = get_eigenvalue(test_pca)
nrow(eig_val)
head(eig_val, 20)
```

```{r}
fviz_eig(test_pca, addlabels = TRUE)
```

```{r}
var <- get_pca_var(test_pca)
var
```

```{r}
# Coordinates - factor loadings
head(var$coord)
```

```{r}
# Correlation between the variable and the PC - almost the same as above. Not sure where the minor difference comes from.
head(var$cor)
```

```{r}
# Cos2: quality on the factore map - squared coordinates: how well the variable is represented by the PC (?)
head(var$cos2)
```

```{r}
# Contributions to the principal components - in percentage: how well the variable represents the PC (?)
head(as.data.frame(var$contrib)%>% 
       mutate(dv = row.names(.)) %>%
       select(dv, everything()) %>%
       arrange(-Dim.1))
```

```{r}
fviz_pca_var(test_pca, geom=c("point"), col.var = "contrib")
```

```{r}
fviz_pca_var(test_pca, geom=c("point"), col.var = "cos2")
```

```{r}
ind <- get_pca_ind(test_pca)
ind
```

```{r}
#Coordinates for individuals - factor scores: how well is a subject represented by a given PC
head(ind$coord)
```

```{r}
#Contribution of each individual to each dimension: how well does a subjcet represent a PC
head(ind$contrib)
```

Note n=522 in this plot.

```{r}
fviz_pca_ind(test_pca, geom=c("point"), col.ind="contrib")
```

```{r}
fviz_pca_ind(test_pca, geom=c("point"), col.ind="cos2")
```

```{r}
retest_data_std[is.na(retest_data_std)]=0

str(predict(test_pca, newdata = retest_data_std))
```

##`stats` package functions

### `princomp` and `prcomp`  
[Difference between the two](https://stats.stackexchange.com/questions/20101/what-is-the-difference-between-r-functions-prcomp-and-princomp)

> The difference between them is nothing to do with the type of PCA they perform, just the method they use. As the help page for prcomp says:

>The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen on the covariance matrix. This is generally the preferred method for numerical accuracy.

>On the other hand, the princomp help page says:

>The calculation is done using eigen on the correlation or covariance matrix, as determined by cor. This is done for compatibility with the S-PLUS result. A preferred method of calculation is to use svd on x, as is done in prcomp."

>So, prcomp is preferred, although in practice you are unlikely to see much difference (for example, if you run the examples on the help pages you should get identical results).

Using the t1 data for retest participants only with 273 variables and 150 subjects `princomp` doesn't would not work because there are more variables than subjects.

```{r eval=FALSE}
fit <- princomp(test_data_std)
```

Changing to `prcomp` works.

```{r}
fit <- prcomp(test_data_std)
```

Explore components  

For ~70% of the variance you'd need ~30 components. Given that there are 14 tasks is this a good decomposition? If you believe that each task measures multiple cognitive processes then maybe.

```{r}
out = data.frame(t(data.frame((summary(fit)$importance)))) %>%
  mutate(PC = row.names(.),
         PC = factor(as.numeric(gsub("PC", "", PC)))) %>%
  select(PC, everything()) 
out
```

Scree plot

```{r}
out %>%
  filter(as.numeric(as.character(PC))<11)%>%
  ggplot(aes(PC, Proportion.of.Variance)) +
  geom_bar(stat="identity")+
  theme(axis.text.x = element_text(angle = 90))
```

Factor loadings

```{r}
data.frame(fit$rotation)
```

Factor scores (n=522)

```{r}
data.frame(fit$x)
```

Plot variables on first two dimensions

```{r}
data.frame(fit$rotation) %>%
  mutate(dv = row.names(.)) %>%
  ggplot(aes(PC1, PC2, label=dv, col=abs(PC1*PC2)))+
  geom_point()+
  # geom_text(hjust = 0)+
  theme_minimal()+
  geom_hline(aes(yintercept=0))+
  geom_vline(aes(xintercept=0))
```

Plot individuals on first two dimensions

```{r}
data.frame(fit$x) %>%
  mutate(sub_id = test_data$sub_id) %>%
  ggplot(aes(PC1, PC2, label=sub_id, col=abs(PC1*PC2)))+
  geom_point()+
  # geom_text(hjust = 0)+
  theme_minimal()+
  geom_hline(aes(yintercept=0))+
  geom_vline(aes(xintercept=0))+
  xlim(c(-16, 16))
```

##`psych` package functions

```{r}
fit_psych <- principal(test_data_std, nfactors=5, rotate="oblimin")
```

Scree plot from `psych` package

```{r}
data.frame(fit_psych$values) %>%
  rename(eig = fit_psych.values) %>%
  arrange(-eig) %>%
  mutate(var_pct = eig/sum(eig)*100,
         pc = 1:n()) %>%
  filter(pc<11)%>%
  ggplot(aes(pc, var_pct))+
  geom_bar(stat="identity")
```

```{r}
as.data.frame(fit_psych$loadings[]) %>%
  mutate(dv = row.names(.)) %>%
  select(dv, everything())
```

```{r}
as.data.frame(fit_psych$scores)
```

#EFA

##`stats` package functions

```{r}
fit_factanal <- factanal(test_data_std, 5, rotate = "promax", n.obs = 0, scores = "regression")
```

##`psych` package functions

```{r eval=F}
#To choose number of factors
parallel <- fa.parallel(test_data_std, fm = 'minres', fa = 'fa')
```

```{r}
#doesn't work - 'ml' is the method used in the ontology paper so don't really want to diverge from it
# tmp = fa(test_data_std, 1, rotate='oblimin', fm='ml', n.obs=0, scores='tenBerge')
tmp = fa(test_data_std, 5, rotate='oblimin', fm='minres', n.obs=0, scores='tenBerge')

```

```{r}
fa_loadings = as.data.frame(tmp$loadings[]) %>%
  mutate(dv = row.names(.))%>%
  select(dv, everything())

fa_loadings[fa_loadings<0.3]=NA

fa_loadings %>% 
  mutate(num_loading = 5-(is.na(MR1)+is.na(MR2)+is.na(MR3)+is.na(MR4)+is.na(MR5))) %>%
  filter(num_loading!=0) %>%
  arrange(-MR1) 
```


```{r}
fa.diagram(tmp)
```

Not a great fit.

```{r}
summary(tmp)
```

It's probably not great to use 
1. untransformed (though scaled)  
2. highly correlated 
variables.

To reduce the number of variables in a data-driven way (instead of just selecting the variables that went in to the ontology paper)
 I'll apply the cleaning methods from the ontology pipeline - transformation of non-normal variables (should be particularly useful for a set of variables with many response times) and dropping variables with r>0.85.

```{r}
clean_test_data = remove_correlated_task_variables(test_data)
```

```{r}
clean_test_data = cbind(sub_id = clean_test_data$sub_id, as.data.frame(apply(clean_test_data[, -which(names(clean_test_data) %in% c("sub_id"))], 2, remove_outliers)))
```

```{r}
numeric_cols = get_numeric_cols()
numeric_cols = numeric_cols[numeric_cols %in% names(clean_test_data) == T]
clean_test_data = transform_remove_skew(clean_test_data, numeric_cols)
```

```{r}
clean_test_data_std = clean_test_data %>% mutate_if(is.numeric, scale)
clean_test_data_std = clean_test_data_std %>% select(-sub_id)

#mean imputation
clean_test_data_std[is.na(clean_test_data_std)]=0

#drop cols with no variance
clean_test_data_std = clean_test_data_std %>%
  select_if(function(col) sd(col) != 0)
```

```{r}
fa_clean = fa(clean_test_data_std, 5, rotate='oblimin', fm='minres', n.obs=0, scores='tenBerge')
```

```{r}
fa_clean_loadings = as.data.frame(fa_clean$loadings[])

fa_clean_loadings[abs(fa_clean_loadings)<0.3]=NA

fa_clean_loadings = fa_clean_loadings %>% 
  mutate(dv = row.names(.))%>%
  select(dv, everything()) %>%
  mutate(num_loading = 5-(is.na(MR1)+is.na(MR2)+is.na(MR3)+is.na(MR4)+is.na(MR5))) %>%
  filter(num_loading!=0) %>%
  arrange(-MR1, -MR2, -MR3, -MR4, -MR5) 

fa_clean_loadings
```


```{r}
fa_clean_loadings %>% 
  select(-num_loading) %>%
  arrange(-MR1, -MR2, -MR3, -MR4, -MR5) %>%
  mutate(order_num = 1:n(),
         dv = reorder(dv, -order_num)) %>%
  select(-order_num) %>%
  gather(Factor, Loading, -dv) %>%
  na.exclude() %>%
  mutate(load_sign = factor(ifelse(Loading>0,"pos","neg"))) %>%
  ggplot(aes(dv, abs(Loading), fill=load_sign))+
  geom_bar(stat = "identity")+
  facet_wrap(~Factor, nrow=1)+
  coord_flip()+
  xlab("")+
  theme(legend.position = "none")+
  ylab("Absolute Loading")

ggsave('fa_diagram.jpeg', device = "jpeg", path = "../output/figures/", width = 10, height = 50, units = "in", limitsize = FALSE, dpi = 100)

```

```{r}
summary(fa_clean)
```

# Concepts

- eigenvalue: amount of variance explained by each PC  

- coordinate/correlation: the correlation between the variable and the PC  
- quality of representation: squared correlation; how well the variable is represented by the PC; variance of variable explained by the PC  
- contribution: varianc of PC explained by the variable

- Factor loadings (aka rotation) (variables)     
- Factor scores (individuals)

# Resources

[PCA Essentials](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/)
[Difference between PCA and EFA](http://www2.sas.com/proceedings/sugi30/203-30.pdf)