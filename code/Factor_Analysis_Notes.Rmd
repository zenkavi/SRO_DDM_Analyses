---
title: 'Factor Analysis Notes'
output:
github_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_DDM_Analyses/code/SRO_DDM_Analyses_Helper_Functions.R')

test_data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_03-29-2018/'

retest_data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-29-2018/'

input_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_DDM_Analyses/input/'

library(FactoMineR)
library(missForest)
library(factoextra)
```

### Load measure labels

```{r}
measure_labels <- read.csv(paste0(input_path, 'measure_labels.csv'))

measure_labels = measure_labels %>% 
  select(-measure_description) %>%
  filter(ddm_task == 1) %>%
  select(-ddm_task) %>%
  filter(rt_acc != "other") %>%
  mutate(dv = as.character(dv),
         overall_difference = factor(overall_difference,levels = c("overall", "difference", "condition"), labels = c("non-contrast", "contrast", "condition")),
         ddm_raw = ifelse(raw_fit == "raw", "raw", "ddm")) %>%
  separate(dv, c("task_group", "var"), sep = "\\.", remove=FALSE, extra = "merge")
```

### Load time 1 data
```{r}
test_data <- read.csv(paste0(test_data_path,'variables_exhaustive.csv'))

test_data_subs = as.character(test_data$X)

test_data = test_data %>%
  select(measure_labels$dv)

test_data$sub_id = test_data_subs

rm(test_data_subs)
```

### Load time 2 data 
```{r}
retest_data <- read.csv(paste0(retest_data_path,'variables_exhaustive.csv'))

retest_data_subs = as.character(retest_data$X)

retest_data = retest_data %>%
  select(measure_labels$dv)

retest_data$sub_id = retest_data_subs

retest_data = retest_data[retest_data$sub_id %in% test_data$sub_id,]

rm(retest_data_subs)
```

### Standardize data and (mean) impute

```{r}
#Standardize datasets
test_data_std = test_data %>% mutate_if(is.numeric, scale)
test_data_std = test_data_std %>% select(-sub_id)

retest_data_std = retest_data %>% mutate_if(is.numeric, scale)
retest_data_std = retest_data_std %>% select(-sub_id)

#mean imputation on standardized data
test_data_std[is.na(test_data_std)]=0
retest_data_std[is.na(retest_data_std)]=0
```

### Clean data

It's probably not great to use 
1. untransformed (though scaled)  
2. highly correlated 
variables.

To reduce the number of variables in a data-driven way (instead of just selecting the variables that went in to the ontology paper)
 I'll apply the cleaning methods from the ontology pipeline - transformation of non-normal variables (should be particularly useful for a set of variables with many response times) and dropping variables with r>0.85.

Remove correlated variables within a task

```{r}
clean_test_data = remove_correlated_task_variables(test_data)
```

Remove outliers (>2.5 SD away)

```{r}
clean_test_data = cbind(sub_id = clean_test_data$sub_id, as.data.frame(apply(clean_test_data[, -which(names(clean_test_data) %in% c("sub_id"))], 2, remove_outliers)))
```

Transform skewed variables

```{r}
numeric_cols = get_numeric_cols()
numeric_cols = numeric_cols[numeric_cols %in% names(clean_test_data) == T]
clean_test_data = transform_remove_skew(clean_test_data, numeric_cols)
```

Drop subject identifier column, mean impute and drop cols with no variance

```{r eval=FALSE}
#Imputation with missForest package - SLOW
test_data_std_miss <- missForest(as.matrix(test_data_std))
```

```{r}
clean_test_data_std = clean_test_data %>% mutate_if(is.numeric, scale)
clean_test_data_std = clean_test_data_std %>% select(-sub_id)

#mean imputation
clean_test_data_std[is.na(clean_test_data_std)]=0

#drop cols with no variance
clean_test_data_std = clean_test_data_std %>%
  select_if(function(col) sd(col) != 0)
```

#Difference between PCA and EFA

>Despite all these similarities, there is a fundamental difference between them: PCA is a linear combination of variables; Factor Analysis is a measurement model of a latent variable.

[Source](https://www.theanalysisfactor.com/the-fundamental-difference-between-principal-component-analysis-and-factor-analysis/)

#PCA

## `FactoMineR` package

Following the tutorial [here](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/)

```{r}
clean_test_pca <- PCA(clean_test_data_std, graph = FALSE, scale.unit = FALSE)
```

```{r}
clean_test_pca
```

Eigenvalues don't go <1 until dimension 90>. 

NOTE: You should have as many eigenvalues as variables (columns) BUT if you fit it to data with more variables than individuals (row) you only have `nrow` eigenvalues.

```{r}
eig_val = get_eigenvalue(clean_test_pca)
nrow(eig_val)
```

```{r}
head(eig_val, 20)
```

Scree plot

```{r}
fviz_eig(clean_test_pca, addlabels = TRUE)
```

```{r}
var <- get_pca_var(clean_test_pca)
var
```

Factor loadings

```{r}
# Coordinates - factor loadings
head(var$coord)
```

Correlation between the variable and the PC - almost the same as above. Not sure where the minor difference comes from.

```{r}
head(var$cor)
```

Cos2: quality on the factore map - squared coordinates: how well the variable is represented by the PC (?)

```{r}
head(var$cos2)
```

Contributions to the principal components - in percentage: how well the variable represents the PC (?)

```{r}
head(as.data.frame(var$contrib)%>% 
       mutate(dv = row.names(.)) %>%
       select(dv, everything()) %>%
       arrange(-Dim.1))
```

Plot variables on first two components and color by their contribution.

```{r}
fviz_pca_var(clean_test_pca, geom=c("point"), col.var = "contrib")
```

Plot variables on first two components and color by their squared loadings.

```{r}
fviz_pca_var(clean_test_pca, geom=c("point"), col.var = "cos2")
```

```{r}
ind <- get_pca_ind(clean_test_pca)
ind
```

Coordinates for individuals - factor scores: how well is a subject represented by a given PC

```{r}
head(ind$coord)
```

Contribution of each individual to each dimension: how well does a subjcet represent a PC

```{r}
head(ind$contrib)
```

Plot individuals on the first two dimensions and color by their contribution. Less structure compared to the plot of variables.

Note n=522 in this plot.

```{r}
fviz_pca_ind(clean_test_pca, geom=c("point"), col.ind="contrib")
```

Plot individuals on the first two dimensions and color by their squared scores.

```{r}
fviz_pca_ind(clean_test_pca, geom=c("point"), col.ind="cos2")
```

Use PCA from test data to predict retest data

```{r}
clean_data_cols = names(clean_test_data_std)
clean_data_cols = gsub(".ReflogTr", "", clean_data_cols)
clean_data_cols = gsub(".logTr", "", clean_data_cols)

clean_data_log_cols = grep("\\.logTr", names(clean_test_data_std), value = T)
clean_data_reflog_cols = grep("\\.ReflogTr", names(clean_test_data_std), value=T)

#Extract the correct dv's from retest data
clean_retest_data_std = retest_data_std %>% select(clean_data_cols)

#Clean extracted retest data the way it has been cleaned for test data
for(i in 1:length(names(clean_retest_data_std))){
  if(names(clean_retest_data_std)[i] %in% gsub(".logTr", "", clean_data_log_cols)){
    clean_retest_data_std[,i]=pos_log(clean_retest_data_std[,i])
    names(clean_retest_data_std)[i] = paste0(names(clean_retest_data_std)[i],".logTr")
    print(paste0("Pos log on ", names(retest_data_std)[i]))
  }
  if(names(clean_retest_data_std)[i] %in% gsub(".ReflogTr", "", clean_data_reflog_cols)){
    clean_retest_data_std[,i]=neg_log(clean_retest_data_std[,i])
    names(clean_retest_data_std)[i] = paste0(names(clean_retest_data_std)[i],".ReflogTr") 
    print(paste0("Neg log on ", names(retest_data_std)[i]))
  }
}

rm(clean_data_cols, clean_data_log_cols, clean_data_reflog_cols,i)

# Check if all names match
# sum(names(clean_retest_data_std) == names(clean_test_data_std))

predict_retest = predict(clean_test_pca, newdata = clean_retest_data_std)
```

```{r}
str(predict_retest)
```

Correlation between factor scores from test and retest

```{r}
data.frame(predict_retest$coord) %>%
  mutate(sub_id = retest_data $sub_id) %>%
  gather(key, value, -sub_id) %>%
  left_join(data.frame(clean_test_pca$ind$coord) %>% 
              mutate(sub_id = clean_test_data$sub_id) %>%
              gather(key, value, -sub_id), by = c("sub_id", "key")) %>%
  ggplot(aes(value.y, value.x))+
  geom_point()+
  geom_abline(aes(slope=1, intercept=0))+
  facet_wrap(~key)+
  xlab("Factor score from test")+
  ylab("Facot score from retest")


```

##`stats` package functions

### `princomp` and `prcomp`  
[Difference between the two](https://stats.stackexchange.com/questions/20101/what-is-the-difference-between-r-functions-prcomp-and-princomp)

> The difference between them is nothing to do with the type of PCA they perform, just the method they use. As the help page for prcomp says:

>The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen on the covariance matrix. This is generally the preferred method for numerical accuracy.

>On the other hand, the princomp help page says:

>The calculation is done using eigen on the correlation or covariance matrix, as determined by cor. This is done for compatibility with the S-PLUS result. A preferred method of calculation is to use svd on x, as is done in prcomp."

>So, prcomp is preferred, although in practice you are unlikely to see much difference (for example, if you run the examples on the help pages you should get identical results).

Using the t1 data for retest participants only with 273 variables and 150 subjects `princomp` doesn't would not work because there are more variables than subjects.

```{r eval=FALSE}
fit <- princomp(test_data_std)
```

Changing to `prcomp` works.

```{r}
fit <- prcomp(clean_test_data_std)
```

Explore components  

For ~70% of the variance you'd need ~30 components. Given that there are 14 tasks is this a good decomposition? If you believe that each task measures multiple cognitive processes then maybe.

```{r}
out = data.frame(t(data.frame((summary(fit)$importance)))) %>%
  mutate(PC = row.names(.),
         PC = factor(as.numeric(gsub("PC", "", PC)))) %>%
  select(PC, everything()) 
out
```

Scree plot

```{r}
out %>%
  filter(as.numeric(as.character(PC))<11)%>%
  ggplot(aes(PC, Proportion.of.Variance)) +
  geom_bar(stat="identity")+
  theme(axis.text.x = element_text(angle = 90))
```

Factor loadings

```{r}
data.frame(fit$rotation)
```

Factor scores (n=522)

```{r}
data.frame(fit$x)
```

Plot variables on first two dimensions

```{r}
data.frame(fit$rotation) %>%
  mutate(dv = row.names(.)) %>%
  ggplot(aes(PC1, PC2, label=dv, col=abs(PC1*PC2)))+
  geom_point()+
  # geom_text(hjust = 0)+
  theme_minimal()+
  geom_hline(aes(yintercept=0))+
  geom_vline(aes(xintercept=0))
```

Plot individuals on first two dimensions

```{r}
data.frame(fit$x) %>%
  mutate(sub_id = test_data$sub_id) %>%
  ggplot(aes(PC1, PC2, label=sub_id, col=abs(PC1*PC2)))+
  geom_point()+
  # geom_text(hjust = 0)+
  theme_minimal()+
  geom_hline(aes(yintercept=0))+
  geom_vline(aes(xintercept=0))+
  xlim(c(-16, 16))
```

##`psych` package functions

```{r}
fit_psych <- principal(clean_test_data_std, nfactors=5, rotate="oblimin")
```

Scree plot from `psych` package

```{r}
data.frame(fit_psych$values) %>%
  rename(eig = fit_psych.values) %>%
  arrange(-eig) %>%
  mutate(var_pct = eig/sum(eig)*100,
         pc = 1:n()) %>%
  filter(pc<11)%>%
  ggplot(aes(pc, var_pct))+
  geom_bar(stat="identity")
```

Factor loadings

```{r}
as.data.frame(fit_psych$loadings[]) %>%
  mutate(dv = row.names(.)) %>%
  select(dv, everything())
```

Factor scores

```{r}
as.data.frame(fit_psych$scores)
```


#EFA

##`stats` package functions

```{r eval=FALSE}
fit_factanal <- factanal(clean_test_data_std, 5, rotate = "promax", n.obs = 0, scores = "regression")
```

##`psych` package functions

```{r eval=F}
#To choose number of factors
parallel <- fa.parallel(clean_test_data_std, fm = 'minres', fa = 'fa')
```

```{r}
#doesn't work - 'ml' is the method used in the ontology paper so don't really want to diverge from it
# tmp = fa(test_data_std, 1, rotate='oblimin', fm='ml', n.obs=0, scores='tenBerge')
tmp = fa(clean_test_data_std, 5, rotate='oblimin', fm='minres', n.obs=0, scores='tenBerge')

```

```{r}
fa_loadings = as.data.frame(tmp$loadings[]) %>%
  mutate(dv = row.names(.))%>%
  select(dv, everything())

fa_loadings[fa_loadings<0.3]=NA

fa_loadings %>% 
  mutate(num_loading = 5-(is.na(MR1)+is.na(MR2)+is.na(MR3)+is.na(MR4)+is.na(MR5))) %>%
  filter(num_loading!=0) %>%
  arrange(-MR1) 
```


```{r}
fa.diagram(tmp)
```

Not a great fit.

```{r}
summary(tmp)
```


```{r}
fa_clean = fa(clean_test_data_std, 5, rotate='oblimin', fm='minres', n.obs=0, scores='tenBerge')
```

```{r}
fa_clean_loadings = as.data.frame(fa_clean$loadings[])

fa_clean_loadings[abs(fa_clean_loadings)<0.3]=NA

fa_clean_loadings = fa_clean_loadings %>% 
  mutate(dv = row.names(.))%>%
  select(dv, everything()) %>%
  mutate(num_loading = 5-(is.na(MR1)+is.na(MR2)+is.na(MR3)+is.na(MR4)+is.na(MR5))) %>%
  filter(num_loading!=0) %>%
  arrange(-MR1, -MR2, -MR3, -MR4, -MR5) 

fa_clean_loadings
```


```{r}
fa_clean_loadings %>% 
  select(-num_loading) %>%
  arrange(-MR1, -MR2, -MR3, -MR4, -MR5) %>%
  mutate(order_num = 1:n(),
         dv = reorder(dv, -order_num)) %>%
  select(-order_num) %>%
  gather(Factor, Loading, -dv) %>%
  na.exclude() %>%
  mutate(load_sign = factor(ifelse(Loading>0,"pos","neg"))) %>%
  ggplot(aes(dv, abs(Loading), fill=load_sign))+
  geom_bar(stat = "identity")+
  facet_wrap(~Factor, nrow=1)+
  coord_flip()+
  xlab("")+
  theme(legend.position = "none")+
  ylab("Absolute Loading")

ggsave('fa_diagram.jpeg', device = "jpeg", path = "../output/figures/", width = 10, height = 50, units = "in", limitsize = FALSE, dpi = 100)

```

```{r}
summary(fa_clean)
```

#Clustering

#`stats` package

```{r}
set.seed(230948)

#clustering people
ind_clusters <- kmeans(clean_test_data_std, 5)

str(ind_clusters)
data.frame(ind_clusters$centers)
```

```{r}
var_clusters <- kmeans(t(clean_test_data_std),5)
```

```{r}
tmp = data_frame(cluster = var_clusters$cluster, dv = names(clean_test_data_std)) %>%
  select(dv, cluster) %>%
  mutate(dv = gsub(".ReflogTr", "", dv),
         dv = gsub(".logTr", "", dv))%>%
  left_join(measure_labels[,c("dv", "task_group","overall_difference","raw_fit","rt_acc", "ddm_raw")], by = "dv")%>%
    mutate(dv = reorder(dv, -cluster))

tmp %>%
  ggplot(aes(dv, cluster, alpha = ddm_raw, fill=rt_acc))+
  geom_bar(stat="identity")+
  coord_flip()+
  xlab("")+
  scale_alpha_discrete(range = c(0.4, 0.8))

ggsave('var_clusters.jpeg', device = "jpeg", path = "../output/figures/", width = 10, height = 50, units = "in", limitsize = FALSE, dpi = 100)

```

```{r}
with(tmp, table(rt_acc, ddm_raw, cluster))
```

#`cluster` package

```{r}
library(cluster)
```

```{r}
clean_test_data_std_dist = stats::dist(t(clean_test_data_std))

hclust_out = hclust(clean_test_data_std_dist)

```

```{r}
library(dendextend)

dend = t(clean_test_data_std) %>%
  dist %>%
  hclust %>%
  as.dendrogram

dend %>%
  plot()

rt_acc_cols <- rainbow_hcl(5)

tmp = data.frame(dv = dend %>% labels) %>%
  mutate(dv2 = dv,
         dv = gsub(".ReflogTr", "", dv),
         dv = gsub(".logTr", "", dv)) %>%
  left_join(measure_labels[,c("dv", "rt_acc")], by="dv") %>%
  mutate(cols = ifelse(rt_acc == "rt", rt_acc_cols[1], ifelse(rt_acc == "accuracy", rt_acc_cols[2], ifelse(rt_acc == "drift rate", rt_acc_cols[3], ifelse(rt_acc == "non-decision", rt_acc_cols[4], ifelse(rt_acc == "threshold", rt_acc_cols[5], NA)))))) %>%
  select(-dv) %>%
  rename(dv = dv2)

tmp[is.na(tmp$cols),]

rt_acc_cols = tmp$cols


dend2 <- dend %>% 
  set("leaves_pch", 19) %>% 
  set("leaves_cex", 1) %>% 
  set("leaves_col", rt_acc_cols)%>% 
  color_labels(col = rt_acc_cols) %>%
  set("labels", rep(NA,419))

plot(dend2, axes=FALSE)
```

# Neuroecon Poster

## PCA on EZ vars

```{r}
ez_vars = grep("EZ", names(clean_test_data_std), value=T)

ez_pca <- PCA(clean_test_data_std %>% select(ez_vars), ncp=3, graph = FALSE, scale.unit = FALSE)

fviz_eig(ez_pca, addlabels = TRUE)
```

Hierarchical clustering of PCA loadings of EZ vars

```{r}
ez_var_low_dim = ez_pca$var$coord

dend = ez_var_low_dim %>%
  dist %>%
  hclust %>%
  as.dendrogram

rt_acc_cols <- rainbow_hcl(3)
overall_diff_cols <- rainbow_hcl(3)

tmp = data.frame(dv = dend %>% labels) %>%
  mutate(dv2 = dv,
         dv = gsub(".ReflogTr", "", dv),
         dv = gsub(".logTr", "", dv)) %>%
  left_join(measure_labels[,c("dv", "rt_acc", "overall_difference")], by="dv") %>%
  mutate(rt_acc_cols = ifelse(rt_acc == "drift rate", rt_acc_cols[1], ifelse(rt_acc == "non-decision", rt_acc_cols[2], ifelse(rt_acc == "threshold", rt_acc_cols[3], NA))),
         overall_diff_cols = ifelse(overall_difference == "non-contrast", overall_diff_cols[1], ifelse(overall_difference == "contrast", overall_diff_cols[2], ifelse(overall_difference == "condition", overall_diff_cols[3], NA)))) %>%
  select(-dv) %>%
  rename(dv = dv2)

dend2 <- dend %>% 
  set("leaves_pch", 19) %>% 
  set("leaves_cex", 1) %>% 
  set("leaves_col", tmp$rt_acc_cols)%>% 
  # color_labels(col = rt_acc_cols) 
  #color_labels(col = tmp$overall_diff_cols) 
# %>%
  set("labels", rep(NA,164))
op <- par(no.readonly = TRUE)
par(mar=c(15, 4, 2, 2) + 0.1)
plot(dend2, axes=FALSE)
par(op)
```



## PCA on HDDM vars

```{r}
hddm_vars = grep("hddm", names(clean_test_data_std), value=T)

hddm_pca <- PCA(clean_test_data_std %>% select(hddm_vars), ncp=3, graph = FALSE, scale.unit = FALSE)

fviz_eig(hddm_pca, addlabels = TRUE)
```

Hierarchical clustering of PCA loadings of HDDM vars

```{r}
hddm_var_low_dim = hddm_pca$var$coord

dend = hddm_var_low_dim %>%
  dist %>%
  hclust %>%
  as.dendrogram

rt_acc_cols <- rainbow_hcl(3)
overall_diff_cols <- rainbow_hcl(3)

tmp = data.frame(dv = dend %>% labels) %>%
  mutate(dv2 = dv,
         dv = gsub(".ReflogTr", "", dv),
         dv = gsub(".logTr", "", dv)) %>%
  left_join(measure_labels[,c("dv", "rt_acc", "overall_difference")], by="dv") %>%
  mutate(rt_acc_cols = ifelse(rt_acc == "drift rate", rt_acc_cols[1], ifelse(rt_acc == "non-decision", rt_acc_cols[2], ifelse(rt_acc == "threshold", rt_acc_cols[3], NA))),
         overall_diff_cols = ifelse(overall_difference == "non-contrast", overall_diff_cols[1], ifelse(overall_difference == "contrast", overall_diff_cols[2], ifelse(overall_difference == "condition", overall_diff_cols[3], NA)))) %>%
  select(-dv) %>%
  rename(dv = dv2)

dend2 <- dend %>% 
  set("leaves_pch", 19) %>% 
  set("leaves_cex", 1) %>% 
  set("leaves_col", tmp$rt_acc_cols)%>% 
  # color_labels(col = rt_acc_cols) 
  color_labels(col = tmp$overall_diff_cols) 
# %>%
#   set("labels", rep(NA,90))
op <- par(no.readonly = TRUE)
par(mar=c(15, 4, 2, 2) + 0.1)
plot(dend2, axes=FALSE)
par(op)
```

Correlation of the factor scores between these two lower dimensional spaces

```{r}
ez_pca$ind$coord

hddm_pca$ind$coord
```

Lower dimensional factor scores are better individual difference factors - compare their reliability

```{r}

```

Then use them for prediction

```{r}

```


# Concepts

- eigenvalue: amount of variance explained by each PC  
- coordinate/correlation: the correlation between the variable and the PC  
- quality of representation: squared correlation; how well the variable is represented by the PC; variance of variable explained by the PC  
- contribution: varianc of PC explained by the variable
- Factor loadings (aka rotation) (variables)     
- Factor scores (individuals)

# Resources

[PCA Essentials](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/)
[Difference between PCA and EFA](http://www2.sas.com/proceedings/sugi30/203-30.pdf)
[K-means clustering hands-on tutorial](https://www.datacamp.com/community/tutorials/k-means-clustering-r)