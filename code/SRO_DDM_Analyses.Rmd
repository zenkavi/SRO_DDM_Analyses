---
title: 'Self Regulation Ontology DDM Analyses'
output:
github_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_DDM_Analyses/code/SRO_DDM_Analyses_Helper_Functions.R')

test_data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_03-29-2018/'

retest_data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-29-2018/'
```

## Loading datasets

The tasks included in this report are:  

```{r}
measure_labels <- read.csv('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/input/measure_labels.csv')

measure_labels = measure_labels %>% select(-measure_description)

measure_labels = measure_labels %>% 
  filter(ddm_task == 1)

measure_labels = measure_labels %>%
  mutate(dv = as.character(dv)) %>%
  separate(dv, c("task_group", "var"), sep = "\\.", remove=FALSE, extra = "merge")

unique(measure_labels$task_group)
```

### Load time 1 data
```{r}
test_data <- read.csv(paste0(retest_data_path,'t1_data/variables_exhaustive.csv'))

test_data_subs = as.character(test_data$X)

test_data = test_data %>%
  select(measure_labels$dv)

test_data$sub_id = test_data_subs

rm(test_data_subs)
```

### Load time 2 data 
```{r}
retest_data <- read.csv(paste0(retest_data_path,'variables_exhaustive.csv'))

retest_data_subs = as.character(retest_data$X)

retest_data = retest_data %>%
  select(measure_labels$dv)

retest_data$sub_id = retest_data_subs

retest_data = retest_data[retest_data$sub_id %in% test_data$sub_id,]

rm(retest_data_subs)
```

### Replace HDDM parameters in t1 data

Since HDDM parameters depend on the sample on which they are fit we refit the model on t1 data for the subjects that have t2 data. Here we replace the HDDM parameters in the current t1 dataset with these refitted values. 

```{r}
hddm_refits <- read.csv(paste0(retest_data_path,'t1_data/hddm_refits_exhaustive.csv'))

test_data_hddm_full_sample <- test_data

tmp = names(test_data_hddm_full_sample)[names(test_data_hddm_full_sample) %in% names(hddm_refits) == FALSE]

hddm_refit_subs = as.character(hddm_refits$X)

hddm_refits = hddm_refits[, c(names(hddm_refits) %in% measure_labels$dv)]

hddm_refits$sub_id <- hddm_refit_subs

rm(hddm_refit_subs)

###RERUN MOTOR SS FOR REFIT (MISSING REACTIVE CONTROL HDDM DRIFT) - after correction this should just be sub_id
#tmp

test_data_hddm_retest_sample = test_data_hddm_full_sample %>%
  select(tmp)

#merge hddm refits to test data
test_data_hddm_retest_sample = merge(test_data_hddm_retest_sample, hddm_refits, by="sub_id")

rm(test_data, hddm_refits, tmp)
```

### Create point estimates

#### Using hddm full sample

```{r}

```

#### Using hddm retest only sample

```{r}

```

### Load bootstrapped reliabilities

```{r}

```

## DDM vs raw reliability overall

```{r}
retest_reliability ~ raw_1_fit_0 + (1|task_group)
```


## Best measure for each task

- Retest reliability for each task: 
-- What is the best measure of individual difference for any measure that has both raw and DDM parameters? 
-- Table each task where rows are each task and cols are mean reliability from raw measures, mean reliability of ddm parameters, name of most reliable measure for that task

## Variance breakdown measure types

- Change in parameter values depending on full vs. retest only sample
-- How much do parameter values change depending on whether the model was fit using the whole sample vs only the retest sample. 

NOTE: The lack of clear systematicity in the changes of the parameter estimates can be difficult to interpret. A systematic change would have meant that the ranking remained the same regardless of the sample size. When the change is non-systematic in this way then a given subject that has high higher drift rates than most of the people in the full sample can appear to have lower drift rates than the same people when fit on the retest sample only. 

Of course these parameter estimates themselves are not very interpretable in their 'goodness' as they don't reflect anything about model fits. To determine the model fits for both cases I have written functions to sample from the posterior predictive and calculate $R^2$'s for each subject.

## Sample size effects

### Parameter stability

### Prameter reliability

- Parameter reliability depending on sample size: in addition to comparing just 150 vs 500

The reliability of a measure depends on the different sources of variability. It would be larger the larger the between subject variability (BSV) is (and the lower within subject, WSV, and error variances, ERV, are). Larger sample sizes do not necessarily imply larger BSV. If there is a true BSV for the whole sample then BSV's of subsamples would have distribution that would be centered around this value with some variance. This variance would be smaller the larger the sample size. Because the distribution is centered around the true BSV the median reliability estimate from 100 samples would not deviate. But the variance of that reliability estimate would. Specifically the variance of the reliability estimates should decrease with increasing sample size.

## Hierarchical estimation consequences

- Is the H in HDDM important? (h vs flat)
-- Change in parameter value?
-- Change in parameter reliability?

## Clustering
-- Do DDM parameters capture similar processes as the raw measures in a given task or do they capture processes that are more similar across tasks?
-- Are these clusters more reliable than using either the raw or the DDM measures alone?

## Prediction
-- Do raw or DDM measures (or factor scores) predict real world outcomes better?

## To Do

Prof. Domingue recommended readings:
file:///Users/zeynepenkavi/Downloads/10.1007%252Fs11336-006-1478-z.pdf
A HIERARCHICAL FRAMEWORK FOR MODELING SPEED AND ACCURACY
ON TEST ITEMS

file:///Users/zeynepenkavi/Downloads/v66i04.pdf
Fitting Diffusion Item Response Theory Models for
Responses and Response Times Using the R
Package diffIRT