---
title: 'Self Regulation Ontology DDM Analyses'
output:
github_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_DDM_Analyses/code/SRO_DDM_Analyses_Helper_Functions.R')

test_data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Complete_03-29-2018/'

retest_data_path = '/Users/zeynepenkavi/Documents/PoldrackLabLocal/Self_Regulation_Ontology/Data/Retest_03-29-2018/'

input_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_DDM_Analyses/input/'
```

## Loading datasets

The tasks included in this report are:  

```{r}
measure_labels <- read.csv(paste0(input_path, 'measure_labels.csv'))

measure_labels = measure_labels %>% 
  select(-measure_description) %>%
  filter(ddm_task == 1) %>%
  select(-ddm_task) %>%
  filter(rt_acc != "other") %>%
  mutate(dv = as.character(dv),
         overall_difference = factor(overall_difference,levels = c("overall", "difference", "condition"), labels = c("non-contrast", "contrast", "condition")),
         ddm_raw = ifelse(raw_fit == "raw", "raw", "ddm")) %>%
  separate(dv, c("task_group", "var"), sep = "\\.", remove=FALSE, extra = "merge")

unique(measure_labels$task_group)
```

### Load time 1 data
```{r}
test_data <- read.csv(paste0(retest_data_path,'t1_data/variables_exhaustive.csv'))

test_data_subs = as.character(test_data$X)

test_data = test_data %>%
  select(measure_labels$dv)

test_data$sub_id = test_data_subs

rm(test_data_subs)
```

### Load time 2 data 
```{r}
retest_data <- read.csv(paste0(retest_data_path,'variables_exhaustive.csv'))

retest_data_subs = as.character(retest_data$X)

retest_data = retest_data %>%
  select(measure_labels$dv)

retest_data$sub_id = retest_data_subs

retest_data = retest_data[retest_data$sub_id %in% test_data$sub_id,]

rm(retest_data_subs)
```

### HDDM parameters in t1 data

Since HDDM parameters depend on the sample on which they are fit we also refit the model on t1 data only for the subjects that have t2 data.

```{r}
hddm_refits <- read.csv(paste0(retest_data_path,'t1_data/hddm_refits_exhaustive.csv'))

test_data_hddm_fullfit <- test_data

tmp = names(test_data_hddm_fullfit)[names(test_data_hddm_fullfit) %in% names(hddm_refits) == FALSE]

hddm_refit_subs = as.character(hddm_refits$X)

hddm_refits = hddm_refits[, c(names(hddm_refits) %in% measure_labels$dv)]

hddm_refits$sub_id <- hddm_refit_subs

rm(hddm_refit_subs)

###RERAN MOTOR SS FOR REFIT (MISSING REACTIVE CONTROL HDDM DRIFT) - still missing; while missing the values from fitting to the full sample are used
# after correction this should just be sub_id
#tmp

test_data_hddm_refit = test_data_hddm_fullfit %>%
  select(tmp)

#merge hddm refits to test data
test_data_hddm_refit = merge(test_data_hddm_refit, hddm_refits, by="sub_id")

rm(test_data, hddm_refits, tmp)
```

### Create reliability point estimates

#### Using hddm full sample

```{r}
#for matching function to work properly assign name back to test_data
test_data  = test_data_hddm_fullfit

numeric_cols = get_numeric_cols()

#Create df of point estimate reliabilities
rel_df_cols = c('icc', 'pearson', 'var_subs', 'var_ind', 'var_resid', 'dv')

rel_df = as.data.frame(matrix(ncol = length(rel_df_cols)))

names(rel_df) = rel_df_cols

for(i in 1:length(numeric_cols)){
  
  cur_dv = numeric_cols[i]
  
  tmp = get_retest_stats(cur_dv, metric = c('icc', 'pearson', 'var_breakdown'))
  
  if(nrow(tmp) == 0){
    tmp[1,]=NA
    tmp$dv = NA
  } else {
    tmp$dv = cur_dv
  }
  
  rel_df = rbind(rel_df, tmp)
  
}

rel_df= rel_df[-which(is.na(rel_df$dv)),]
```

Reassign df's and clean up

```{r}
rel_df_fullfit = rel_df
rm(test_data, rel_df, tmp, i, numeric_cols, rel_df_cols)
```

#### Using hddm retest only sample

```{r}
#for matching function to work properly assign name back to test_data
test_data  = test_data_hddm_refit

numeric_cols = get_numeric_cols()

#Create df of point estimate reliabilities
rel_df_cols = c('icc', 'pearson', 'var_subs', 'var_ind', 'var_resid', 'dv')

rel_df = as.data.frame(matrix(ncol = length(rel_df_cols)))

names(rel_df) = rel_df_cols

for(i in 1:length(numeric_cols)){
  
  cur_dv = numeric_cols[i]
  
  tmp = get_retest_stats(cur_dv, metric = c('icc', 'pearson', 'var_breakdown'))
  
  if(nrow(tmp) == 0){
    tmp[1,]=NA
    tmp$dv = NA
  } else {
    tmp$dv = cur_dv
  }
  
  rel_df = rbind(rel_df, tmp)
  
}

rel_df= rel_df[-which(is.na(rel_df$dv)),]
```

Reassign df's and clean up

```{r}
rel_df_refit = rel_df
rm(test_data, rel_df, tmp, i, numeric_cols, rel_df_cols)
```

### Load bootstrapped reliabilities

Extract ddm vars only  

Using full sample fits' reliabilities  

```{r}
fullfit_boot_df <- read.csv(gzfile(paste0(retest_data_path,'bootstrap_merged.csv.gz')), header=T)

fullfit_boot_df = process_boot_df(fullfit_boot_df)

fullfit_boot_df = fullfit_boot_df[fullfit_boot_df$dv %in% measure_labels$dv,]
```

Refits bootstrapped reliabilities  

```{r}
refit_boot_df = read.csv(gzfile(paste0(retest_data_path,'refits_bootstrap_merged.csv.gz')), header=T)

refit_boot_df = process_boot_df(refit_boot_df)
```

## T1 HDDM parameters

Before going on with the rest of the report first decide on whether there are significant differences in the distributions of the HDDM parameter estimates and their reliabilities depending on whether they are fit on the full sample (n=552) or retest sample (n=150) for t1 data.  
### Parameter stability

Differences in distributions: using scaled differences

```{r}
hddm_pars_fullfit = test_data_hddm_fullfit %>%
  select(sub_id, unique(refit_boot_df$dv)) %>%
  mutate(hddm_sample = "fullfit")

hddm_pars_refit = test_data_hddm_refit %>%
  select(sub_id, unique(refit_boot_df$dv)) %>%
  mutate(hddm_sample = "refit")

hddm_pars = rbind(hddm_pars_fullfit, hddm_pars_refit)
rm(hddm_pars_fullfit, hddm_pars_refit)

hddm_pars = hddm_pars %>%
  gather(dv, value, -sub_id, -hddm_sample) %>%
  spread(hddm_sample, value) %>%
  mutate(diff = fullfit - refit) %>%
  group_by(dv) %>%
  mutate(scaled_diff = scale(diff)) %>%
  select(sub_id, dv, scaled_diff) %>%
  left_join(measure_labels[,c("dv", "rt_acc")], by = "dv") %>%
  filter(rt_acc %in% c("rt", "accuracy") == FALSE) %>%
  na.omit()

hddm_pars %>%
  ggplot(aes(scaled_diff))+
  geom_density(aes(fill = dv), alpha = 0.2, color = NA)+
  geom_density(fill = "black", alpha = 1, color=NA)+
  theme(legend.position = "none")+
  xlab("Scaled difference of HDDM parameter estimate (full - refit)")
```

Does the distribution of scaled difference scores (between using n=150 or n=552) have a mean different than 0 allowing for random effects of parameter accounting for the different types of parameters? No.

```{r}
summary(lmer(scaled_diff ~ rt_acc + (1|dv), hddm_pars))
# Same result
# summary(MCMCglmm(scaled_diff ~ rt_acc, random = ~ dv, data=hddm_pars))
```

### Parameter reliability

Are there differences in reliability depending which sample the HDDM parameters are estimated from? No.

```{r}
hddm_rels_fullfit = rel_df_fullfit %>%
  filter(dv %in% unique(refit_boot_df$dv)) %>%
  select(dv, icc)

hddm_rels_refit = rel_df_refit %>%
  filter(dv %in% unique(refit_boot_df$dv)) %>%
  select(dv, icc) 

hddm_rels = hddm_rels_fullfit %>%
  left_join(hddm_rels_refit, by = "dv") %>%
  rename(fullfit = icc.x, refit = icc.y) %>%
  left_join(measure_labels[,c("dv", "rt_acc")], by = "dv") %>%
  filter(rt_acc %in% c("rt", "accuracy") == FALSE)

rm(hddm_rels_fullfit, hddm_rels_refit)

hddm_rels %>%
  ggplot(aes(fullfit, refit, col=rt_acc))+
  geom_point()+
  geom_abline(slope=1, intercept = 0)+
  xlab("Reliability of HDDM params using n=552")+
  ylab("Reliability of HDDM params using n=150")+
  theme(legend.title = element_blank())
```

Do hddm parameters differ in their reliability depending on whether they are derived from n=150 or n=552? No.

```{r}
hddm_rels = hddm_rels %>%
  gather(sample, value, -dv, -rt_acc)

summary(lmer(value ~ sample*rt_acc + (1|dv), hddm_rels))
```

You should compare model fits using either sample. That would tell you which estimates explain observed data better though given the lack of difference in parameter estimates I don't expect these to differ either.

Conclusion: Going to use parameter estimates from refits to retest sample only for t1 data in the rest of this report because I think they are more comparable to the parameter estimates from t2.

Clean up

```{r}
rm(hddm_pars, hddm_rels)

boot_df = fullfit_boot_df %>%
  filter(dv %in% unique(refit_boot_df$dv) == FALSE)

boot_df = rbind(boot_df, refit_boot_df)

rm(fullfit_boot_df, refit_boot_df)

rel_df = rel_df_refit

rm(rel_df_fullfit, rel_df_refit)

test_data = test_data_hddm_refit

rm(test_data_hddm_fullfit, test_data_hddm_refit)
```

## DDM vs raw reliability overall

Data wrangling df's containing point estimate reliabilities and bootstrapped reliabilities

```{r}
rel_df = rel_df %>%
  select(dv, icc, var_subs, var_ind, var_resid) %>%
  left_join(measure_labels[,c("dv", "task_group","overall_difference","raw_fit","rt_acc", "ddm_raw")], by = "dv")

boot_df = boot_df %>%
  select(dv, icc, var_subs, var_ind, var_resid) %>%
  left_join(measure_labels[,c("dv", "task_group","overall_difference","raw_fit","rt_acc", "ddm_raw")], by = "dv")
```

Plot reliability point estimates comparing DDM measures to raw measures faceting for contrast measures.

```{r}
rel_df %>%
  mutate(rt_acc = as.character(rt_acc)) %>%
  ggplot(aes(factor(raw_fit, levels = c("raw", "EZ", "hddm"), labels=c("Raw", "EZ-diffusion", "Hierarchical diffusion")), icc, fill=factor(rt_acc, levels = c("rt","accuracy", "drift rate", "threshold", "non-decision"), labels=c("Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))))+
  geom_boxplot()+
  facet_wrap(~overall_difference)+
  ylab("ICC")+
  xlab("")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom')#+
  # guides(fill = guide_legend(ncol = 2, byrow=F))

#ADD FIX_DDM_LEGEND
#fix_ddm_legend(ddm_point_plot)

#rm(mylegend, ddm_point_plot)
```

Plot averaged bootstrapped reliability estimates per measure comparing DDM measures to raw measures faceting for contrast measures.

```{r}
boot_df %>%
  group_by(dv) %>%
  summarise(mean_icc = mean(icc),
            rt_acc = unique(rt_acc),
            overall_difference = unique(overall_difference),
            raw_fit = unique(raw_fit)) %>%
  mutate(rt_acc = as.character(rt_acc)) %>%
  ggplot(aes(factor(raw_fit, levels = c("raw", "EZ", "hddm"), labels=c("Raw", "EZ-diffusion", "Hierarchical diffusion")), mean_icc, fill=factor(rt_acc, levels = c("rt","accuracy", "drift rate", "threshold", "non-decision"), labels=c("Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))))+
  geom_boxplot()+
  facet_wrap(~overall_difference)+
  ylab("ICC")+
  xlab("")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom')#+
  # guides(fill = guide_legend(ncol = 2, byrow=F))

#ADD FIX_DDM_LEGEND
#fix_ddm_legend(ddm_boot_plot)

#rm(mylegend, ddm_boot_plot)
```

Model testing if the reliability of raw measures differs from that of ddm estimates and if contrast measures differ from non-contrast measures.

Checking if both fixed effects of raw vs ddm and contrast vs non-contrast as well as their interaction is necessary.

Conclusion: Interactive model is best.

```{r}
mer1 = lmer(icc ~ ddm_raw + (1|dv), boot_df %>% filter(rt_acc != "other"))
mer1a = lmer(icc ~ overall_difference + (1|dv), boot_df %>% filter(rt_acc != "other"))
mer2 = lmer(icc ~ ddm_raw + overall_difference + (1|dv), boot_df %>% filter(rt_acc != "other"))
mer3 = lmer(icc ~ ddm_raw * overall_difference + (1|dv), boot_df %>% filter(rt_acc != "other"))
anova(mer1, mer2, mer3)
anova(mer1a, mer2, mer3)
```

```{r}
rm(mer1, mer2, mer1a)
```

Raw measures do not significantly differ from ddm parameters in their reliability but non-contrast measures are significantly more reliable compared to contrast and condition measures.

```{r}
summary(mer3)
```

## Best measure for each task

What is the best measure of individual difference for any measure that has both raw and DDM parameters? 

Even though overall the ddm parameters are not significantly less reliable the most reliable measure is more frequently a raw measure. There are some examples of an EZ estimate being the best for a task as well. Regardless of raw vs ddm the best measure is always a non-contrast measure. 

```{r}
rel_df %>%
  group_by(task_group) %>%
  filter(icc == max(icc)) %>%
  select(task_group, everything())
```

## Variance breakdown measure types

Converting variance estimates for each measure to percentage of all variance for comparability.

```{r}
rel_df = rel_df %>%
  mutate(var_subs_pct = (var_subs/(var_subs+var_ind+var_resid))*100,
         var_ind_pct  = (var_ind/(var_subs+var_ind+var_resid))*100,
         var_resid_pct = (var_resid/(var_subs+var_ind+var_resid))*100)

boot_df = boot_df %>%
  mutate(var_subs_pct = (var_subs/(var_subs+var_ind+var_resid))*100,
         var_ind_pct  = (var_ind/(var_subs+var_ind+var_resid))*100,
         var_resid_pct = (var_resid/(var_subs+var_ind+var_resid))*100)
```

```{r message=FALSE, warning=FALSE}
rel_df %>%
  group_by(rt_acc, overall_difference, raw_fit) %>%
  summarise(mean_var_subs_pct = mean(var_subs_pct),
            mean_var_ind_pct = mean(var_ind_pct),
            mean_var_resid_pct = mean(var_resid_pct),
            sem_var_subs_pct = sem(var_subs_pct),
            sem_var_ind_pct = sem(var_ind_pct),
            sem_var_resid_pct = sem(var_resid_pct)) %>%
  ggplot(aes(factor(raw_fit, levels = c("raw", "EZ", "hddm"), labels=c("Raw", "EZ-diffusion", "Hierarchical diffusion")), mean_var_subs_pct, shape=factor(rt_acc, levels = c("rt","accuracy", "drift rate", "threshold", "non-decision"), labels=c("Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))))+
  geom_point(position=position_dodge(width=0.75), size = 5)+
  geom_errorbar(aes(ymin = mean_var_subs_pct - sem_var_subs_pct, ymax = mean_var_subs_pct + sem_var_subs_pct), position=position_dodge(width=0.75))+
  facet_wrap(~overall_difference)+
  ylab("% of between subjects variance")+
  xlab("")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom')
```

```{r message=FALSE, warning=FALSE}
rel_df %>%
  group_by(rt_acc, overall_difference, raw_fit) %>%
  summarise(mean_var_subs_pct = mean(var_subs_pct),
            mean_var_ind_pct = mean(var_ind_pct),
            mean_var_resid_pct = mean(var_resid_pct),
            sem_var_subs_pct = sem(var_subs_pct),
            sem_var_ind_pct = sem(var_ind_pct),
            sem_var_resid_pct = sem(var_resid_pct)) %>%
  ggplot(aes(factor(raw_fit, levels = c("raw", "EZ", "hddm"), labels=c("Raw", "EZ-diffusion", "Hierarchical diffusion")), mean_var_resid_pct, shape=factor(rt_acc, levels = c("rt","accuracy", "drift rate", "threshold", "non-decision"), labels=c("Response Time", "Accuracy","Drift Rate", "Threshold", "Non-decision"))))+
  geom_point(position=position_dodge(width=0.75), size = 5)+
  geom_errorbar(aes(ymin = mean_var_resid_pct - sem_var_resid_pct, ymax = mean_var_resid_pct + sem_var_resid_pct), position=position_dodge(width=0.75))+
  facet_wrap(~overall_difference)+
  ylab("% of residual variance")+
  xlab("")+
  theme(legend.title = element_blank(),
        legend.position = 'bottom')
```

Model testing if the percentage of between subjects variance of raw measures differs from that of ddm estimates and if contrast measures differ from non-contrast measures.

Checking if both fixed effects of raw vs ddm and contrast vs non-contrast as well as their interaction is necessary.

Conclusion: Model with fixed effects for both is best.

```{r}
mer1 = lmer(var_subs_pct ~ ddm_raw + (1|dv), boot_df %>% filter(rt_acc != "other"))
mer1a = lmer(var_subs_pct ~ overall_difference + (1|dv), boot_df %>% filter(rt_acc != "other"))
mer2 = lmer(var_subs_pct ~ ddm_raw +  overall_difference + (1|dv), boot_df %>% filter(rt_acc != "other"))
mer3 = lmer(var_subs_pct ~ ddm_raw *  overall_difference + (1|dv), boot_df %>% filter(rt_acc != "other"))
anova(mer1, mer2, mer3)
anova(mer1a, mer2, mer3)
```

```{r}
rm(mer1, mer1a, mer3)
```

Contrast measures have lower between subjects variability (ie are worse individual difference measures). Raw and ddm measures do not differ significantly.

```{r}
summary(mer2)
```

Model testing if the percentage of residual variance of raw measures differs from that of ddm estimates and if contrast measures differ from non-contrast measures.

Checking if both fixed effects of raw vs ddm and contrast vs non-contrast as well as their interaction is necessary.

Conclusion: Interactive model is best

```{r}
mer1 = lmer(var_resid_pct ~ ddm_raw + (1|dv), boot_df %>% filter(rt_acc != "other"))
mer1a = lmer(var_resid_pct ~ overall_difference + (1|dv), boot_df %>% filter(rt_acc != "other"))
mer2 = lmer(var_resid_pct ~ ddm_raw + overall_difference + (1|dv), boot_df %>% filter(rt_acc != "other"))
mer3 = lmer(var_resid_pct ~ ddm_raw * overall_difference + (1|dv), boot_df %>% filter(rt_acc != "other"))
anova(mer1, mer2, mer3)
anova(mer1a, mer2, mer3)
```

```{r}
rm(mer1, mer1a, mer2)
```

Both contrast and condition measures have higher residual variance. Raw and ddm measures do not differ.

```{r}
summary(mer3)
```

```{r}
rm(mer3)
```

## Sample size effects on reliability

Differences in DDM parameter reliability for t1 data using either n=552 or n=150 were reported above under [T1 HDDM parameters](https://zenkavi.github.io/SRO_DDM_Analyses/output/reports/SRO_DDM_Analyses.nb.html#t1_hddm_parameters). No meaningful differences exist between these two sample sizes.

But even 150 is a large sample size for psychological studies, especially forced choice reaction time tasks that are included in this report. Here we look at how the reliability for raw and ddm measures change for sample sizes that are more common in studies using these tasks (25, 50, 75, 100, 125, 150)

Note: Not refitting HDDM's for each of these sample sizes since a. there were no differences in parameter stability for n=150 vs 552 and b. a more comprehensive comparison using non-hierarchical estimates and model fit indices will follow. *[revisit this - 150 and 552 might be too large to lead to changes in parameter estimates but smaller samples that are more common in psych studies might sway estimates more]*

Note: Some variables do not have enough variance to calculate reliability for difference sample sizes. These variables are:  
>stroop.post_error_slowing  
>simon.std_rt_error  
>shape_matching.post_error_slowing  
>directed_forgetting.post_error_slowing  
>choice_reaction_time.post_error_slowing  
>choice_reaction_time.std_rt_error  
>dot_pattern_expectancy.post_error_slowing  
>motor_selective_stop_signal.go_rt_std_error  
>motor_selective_stop_signal.go_rt_error  
>attention_network_task.post_error_slowing  
>recent_probes.post_error_slowing  
>simon.post_error_slowing  
>dot_pattern_expectancy.BY_errors  

```{r}
rel_df_sample_size = read.csv(gzfile(paste0(input_path, 'rel_df_sample_size.csv.gz')))

#Check if all vars are there
# numeric_cols[which(numeric_cols %in% unique(rel_df_sample_size$dv) == FALSE)]

rel_df_sample_size = rel_df_sample_size %>%
  left_join(measure_labels[,c("dv", "task_group","overall_difference","raw_fit","rt_acc","ddm_raw")], by = "dv") %>%
  mutate(var_subs_pct = (var_subs/(var_subs+var_ind+var_resid))*100,
         var_ind_pct  = (var_ind/(var_subs+var_ind+var_resid))*100,
         var_resid_pct = (var_resid/(var_subs+var_ind+var_resid))*100)

rel_df_sample_size_summary = rel_df_sample_size %>% 
  group_by(dv, sample_size, ddm_raw, overall_difference) %>%
  summarise(mean_icc = mean(icc),
            sem_icc = sem(icc), 
            mean_var_subs_pct = mean(var_subs_pct),
            sem_var_subs_pct = sem(var_subs_pct),
            mean_var_ind_pct = mean(var_ind_pct),
            sem_var_ind_pct = sem(var_ind_pct),
            mean_var_resid_pct = mean(var_resid_pct),
            sem_var_resid_pct = sem(var_resid_pct))

tmp = rel_df_sample_size %>% 
  group_by(sample_size) %>%
  summarise(mean_icc = mean(icc,na.rm=T),
            sem_icc = sem(icc), 
            mean_var_subs_pct = mean(var_subs_pct,na.rm=T),
            sem_var_subs_pct = sem(var_subs_pct),
            mean_var_ind_pct = mean(var_ind_pct, na.rm=T),
            sem_var_ind_pct = sem(var_ind_pct),
            mean_var_resid_pct = mean(var_resid_pct,na.rm=T),
            sem_var_resid_pct = sem(var_resid_pct))
```

Does the mean reliability change with sample size?

Yes. The larger the sample size the more reliable is a given measure on average. The largest increase in reliability is when shifting from 25 to 50 subjects. This is important because many studies using these measures have sample sizes <50 per group.

```{r}
rel_df_sample_size_summary %>%
  na.omit() %>%
  ggplot(aes(factor(sample_size), mean_icc))+
  geom_line(aes(group = dv), alpha = 0.1)+
  geom_line(data = tmp,aes(factor(sample_size),mean_icc, group=1), color="purple", size=1)+
  geom_point(data = tmp,aes(factor(sample_size),mean_icc), color="purple", size=2)+
  geom_errorbar(data = tmp,aes(ymin=mean_icc-sem_icc, ymax = mean_icc+sem_icc), color="purple", width = 0.1)+
  ylab("Mean reliability of 100 samples of size n")+
  xlab("Sample size")+
  ylim(-1,1)
```

```{r}
summary(lmer(icc ~ factor(sample_size) + (1|dv) + (1|iteration), rel_df_sample_size))
```

Does the change in reliabiliity with sample size vary by variable type?

The changes do not differ by raw vs. ddm measures. It does differ by whether the measure is a contrast measure: For contrast measures all increases in reliability with sample size are larger than the increases for non-contrast measures. This implies that larger sample sizes are even more crucial for studies using contrast measures as trait-level measures.

```{r}
tmp = rel_df_sample_size %>% 
  group_by(sample_size, ddm_raw, contrast) %>%
  summarise(mean_icc = mean(icc),
            sem_icc = sem(icc), 
            mean_var_subs_pct = mean(var_subs_pct),
            sem_var_subs_pct = sem(var_subs_pct),
            mean_var_ind_pct = mean(var_ind_pct),
            sem_var_ind_pct = sem(var_ind_pct),
            mean_var_resid_pct = mean(var_resid_pct),
            sem_var_resid_pct = sem(var_resid_pct))
```

```{r}
rel_df_sample_size_summary %>%
  ggplot(aes(sample_size, mean_icc))+
  geom_line(aes(group = dv, color=ddm_raw), alpha = 0.1)+
  geom_line(data = tmp, aes(sample_size,mean_icc, color=ddm_raw))+
  geom_point(data = tmp, aes(sample_size,mean_icc, color=ddm_raw))+
  geom_errorbar(data = tmp,aes(ymin=mean_icc-sem_icc, ymax = mean_icc+sem_icc), color="red", width = 0.1)+
  facet_wrap(~contrast)+
  ylab("Mean reliability of 100 samples of size n")+
  xlab("Sample size")+
  theme(legend.title = element_blank(),
        legend.position = "bottom")
```

```{r}
summary(lmer(icc ~ factor(sample_size) * contrast + (1|dv) + (1|iteration), rel_df_sample_size))
```

Does variability of reliability change with sample size?

Yes. Variability of reliability decreases with sample size. It does so more strongly for contrast measures.

```{r}
rel_df_sample_size_summary %>%
  ggplot(aes(sample_size, sem_icc))+
  geom_line(aes(group = dv, color=ddm_raw), alpha = 0.1)+
  geom_line(data = tmp, aes(sample_size,sem_icc, color=ddm_raw))+
  geom_point(data = tmp, aes(sample_size,sem_icc, color=ddm_raw))+
  facet_wrap(~contrast)+
  ylab("Standard error of mean of reliability \n of 100 samples of size n")+
  xlab("Sample size")+
  theme(legend.title = element_blank(),
        legend.position = "bottom")
```

```{r}
summary(lmer(sem_icc ~ factor(sample_size) * contrast + (1|dv), rel_df_sample_size_summary))
```

Does between subjects variance change with sample size?

Yes. Between subjects variance decreases with sample size. This is more pronounced for non-contrast measures.

This goes against my intuitions. Looking at the change in between subjects percentage of individual measures' there seems to be a lot of inter-measure variance (more pronounced below for within subject variance). I'm not sure if there is something in common for the measures that show increasing between subjects variability with sample size and that separates them from those that show decreasing between subjects variability with sample size (the slight majority).

```{r}
rel_df_sample_size_summary %>%
  ggplot(aes(sample_size, mean_var_subs_pct))+
  geom_line(aes(group = dv, color=ddm_raw), alpha = 0.1)+
  geom_line(data = tmp, aes(sample_size,mean_var_subs_pct, color=ddm_raw))+
  geom_point(data = tmp, aes(sample_size,mean_var_subs_pct, color=ddm_raw))+
  facet_wrap(~contrast)+
  ylab("Mean percentage of \n between subjects variance \n of 100 samples of size n")+
  xlab("Sample size")+
  theme(legend.title = element_blank(),
        legend.position = "bottom")
```

```{r}
summary(lmer(var_subs_pct ~ factor(sample_size) * contrast + (1|dv) + (1|iteration), rel_df_sample_size))
```

Does within subjects variance change with sample size?

Yes. This again goes against my intuition but here the inter-meausre differences are even more pronounced. There appears to be some measures for which the change in two measurements at different time points is larger the more subjects are tested and those that show a smaller decrease in within subject variance with larger sample sizes. I still don't know if these two types of measures have anything that distinguishes them.

```{r}
rel_df_sample_size_summary %>%
  ggplot(aes(sample_size, mean_var_ind_pct))+
  geom_line(aes(group = dv, color=ddm_raw), alpha = 0.1)+
  geom_line(data = tmp, aes(sample_size,mean_var_ind_pct, color=ddm_raw))+
  geom_point(data = tmp, aes(sample_size,mean_var_ind_pct, color=ddm_raw))+
  facet_wrap(~contrast)+
  ylab("Mean percentage of \n within subjects variance \n of 100 samples of size n")+
  xlab("Sample size")+
  theme(legend.title = element_blank(),
        legend.position = "bottom")
```

```{r}
summary(lmer(var_ind_pct ~ factor(sample_size) * contrast + (1|dv) + (1|iteration), rel_df_sample_size))
```

Does residual variance change with sample size?

```{r}
rel_df_sample_size_summary %>%
  ggplot(aes(sample_size, mean_var_resid_pct))+
  geom_line(aes(group = dv, color=ddm_raw), alpha = 0.1)+
  geom_line(data = tmp, aes(sample_size,mean_var_resid_pct, color=ddm_raw))+
  geom_point(data = tmp, aes(sample_size,mean_var_resid_pct, color=ddm_raw))+
  facet_wrap(~contrast)+
  ylab("Mean percentage of residual variance \n of 100 samples of size n")+
  xlab("Sample size")+
  theme(legend.title = element_blank(),
        legend.position = "bottom")
```

```{r}
summary(lmer(var_resid_pct ~ factor(sample_size) * contrast + (1|dv) + (1|iteration), rel_df_sample_size))
```

Conclusion: Larger samples are better for reliability but not necessarily always for the same reasons; for some variables this is due to increasing between subjects variance while for others it's due to decreasing residual variance (?).

```{r}
rm(rel_df_sample_size, rel_df_sample_size_summary)
```

## Hierarchical estimation consequences

- Is the H in HDDM important? (h vs flat)

```{r}
retest_hddm_flat = read.csv(paste0(retest_data_path,'retest_hddm_flat.csv'))

retest_hddm_flat = retest_hddm_flat %>% rename(sub_id = subj_id)

test_hddm_flat = read.csv(paste0(retest_data_path,'/t1_data/t1_hddm_flat.csv'))

test_hddm_flat = test_hddm_flat %>% rename(sub_id = subj_id)

# Check if all the variables are there (no for now)
sum(names(retest_hddm_flat) %in% numeric_cols) == length(names(retest_hddm_flat))
names(retest_hddm_flat)[which(names(retest_hddm_flat) %in% numeric_cols == FALSE)]

sum(names(test_hddm_flat) %in% numeric_cols) == length(names(test_hddm_flat))
names(test_hddm_flat)[which(names(test_hddm_flat) %in% numeric_cols == FALSE)]
```

### Change in parameter value

Standardized differences? Raw difference
Plot raw difference for retest vs test for each of 3 parameters
Percent change - using hierarchical as baseline: what percent does the flat parameter change compared to the hierarchical

```{r}
#Should not be necessary later
common_cols = names(retest_hddm_flat)[names(retest_hddm_flat) %in% names(retest_data)]

retest_hddm_hier = retest_data %>% select(common_cols) %>% mutate(hddm="hierarchical")
test_hddm_hier = test_data %>% select(common_cols)  %>% mutate(hddm="hierarchical")
retest_hddm_flat = retest_hddm_flat %>% select(common_cols) %>% mutate(hddm="flat")
test_hddm_flat = test_hddm_flat %>% select(common_cols) %>% mutate(hddm="flat")

retest_flat_difference = rbind(retest_hddm_hier, retest_hddm_flat)
retest_flat_difference = retest_flat_difference %>%
  gather(dv, value, -sub_id, -hddm) %>%
  spread(hddm, value) %>%
  mutate(diff_pct = (hierarchical - flat)/hierarchical*100,
         time = "retest",
         par = ifelse(grepl("drift", dv), "drift", ifelse(grepl("thresh", dv), "thresh", ifelse(grepl("non_decision", dv), "non_decision", NA))))

test_flat_difference = rbind(test_hddm_hier, test_hddm_flat)
test_flat_difference = test_flat_difference %>%
  gather(dv, value, -sub_id, -hddm) %>%
  spread(hddm, value) %>%
  mutate(diff_pct = (hierarchical - flat)/hierarchical*100,
         time = "test",
         par = ifelse(grepl("drift", dv), "drift", ifelse(grepl("thresh", dv), "thresh", ifelse(grepl("non_decision", dv), "non_decision", NA))))

flat_difference = rbind(test_flat_difference, retest_flat_difference)

flat_difference %>%
  ggplot(aes(diff_pct))+
  geom_histogram()+
  facet_grid(factor(time, levels = c("test", "retest"),labels = c("test", "retest")) ~ par, scales="free")
```

Is the average percentage change different than 0? No.
[RUN MODEL TO COMPARE TO 0. NOT TO BASELINE PARAMETER]

```{r}
summary(lmer(diff_pct ~ time*par+(1|dv), flat_difference %>% mutate(pars = factor(par, levels = c("thresh", "drift", "non_decision")))))
```

Do people change in the same way? Mostly.

```{r}
flat_difference %>%
  ggplot(aes(hierarchical, flat))+
  geom_point()+
  geom_abline(aes(intercept=0, slope=1), color="red")+
  facet_grid(factor(time, levels = c("test", "retest"),labels = c("test", "retest")) ~ par)
```

### Change in parameter reliability

```{r}
test_data_150 = test_data
retest_data_150 = retest_data

test_data = test_hddm_flat
retest_data = retest_hddm_flat
```

```{r}
rel_df_cols = c('icc', 'var_subs', 'var_ind', 'var_resid', 'dv')

rel_df_flat = as.data.frame(matrix(ncol = length(rel_df_cols)))

names(rel_df_flat) = rel_df_cols

numeric_cols = get_numeric_cols()

for(i in 1:length(numeric_cols)){
  
  dv = numeric_cols[i]
  tmp = get_retest_stats(dv, metric = c('icc', 'var_breakdown'))
  tmp$dv = dv
  rel_df_flat = rbind(rel_df_flat, tmp)
}

rel_df_flat = rel_df_flat[-which(is.na(rel_df_flat$dv)),]
```

```{r}
test_data = test_data_150
retest_data = retest_data_150
rm(test_data_150, retest_data_150)
```

```{r}
rel_df_flat = rel_df_flat %>%
  left_join(rel_df[,c("dv", "icc", "rt_acc", "contrast")], by = "dv") 

rel_df_flat %>%
  ggplot(aes(icc.y, icc.x))+
  geom_point()+
  geom_abline(aes(slope=1, intercept = 0), color="red")+
  facet_wrap(~rt_acc)+
  xlab("Hierarchical reliability")+
  ylab("Flat reliability")
```

```{r}
with(rel_df_flat %>% filter(rt_acc == "drift rate"), t.test(icc.x, icc.y, paired=T))
with(rel_df_flat %>% filter(rt_acc == "threshold"), t.test(icc.x, icc.y, paired=T))
with(rel_df_flat %>% filter(rt_acc == "non-decision"), t.test(icc.x, icc.y, paired=T))
```

## Clustering

-- Do DDM parameters capture similar processes as the raw measures in a given task or do they capture processes that are more similar across tasks?
(If the former they would be less useful than if the latter.)  

This could be analyzed with factor analysis but there are more variables than observations so as a first pass we'll explore correlations.    

For this we correlate each DDM measure with:    
1. raw measures within task  
2. other ddm measures across tasks  

```{r}
#Standardize datasets
test_data_std = test_data %>% mutate_if(is.numeric, scale)
test_data_std = test_data_std %>% select(-sub_id)

retest_data_std = retest_data %>% mutate_if(is.numeric, scale)
retest_data_std = retest_data_std %>% select(-sub_id)

#Get correlation table for test data and melt into long form
test_data_cor = data.frame(cor(test_data_std, use="complete.obs"))
test_data_cor = test_data_cor %>%
  mutate(dv = row.names(.)) %>%
  gather(key, value, -dv) %>%
  filter(value != 1 & duplicated(value)==FALSE)

#Add measure type info to long form correlation table
test_data_cor = test_data_cor %>%
  left_join(rel_df %>% select(dv, task_group, raw_fit, rt_acc, ddm_raw), by = "dv") %>%
  rename(var_1 = dv, dv = key, task_group_1 = task_group, raw_fit_1 = raw_fit, rt_acc_1 = rt_acc, ddm_raw_1 = ddm_raw) %>%
    left_join(rel_df %>% select(dv, task_group, raw_fit, rt_acc, ddm_raw), by = "dv") %>%
  rename(var_2 = dv, task_group_2 = task_group, raw_fit_2 = raw_fit, rt_acc_2 = rt_acc, ddm_raw_2 = ddm_raw) %>%
  #task-task: if the measures are from the same task and one of them is a ddm measure while the other is raw
  #ddm-ddm: if the measures are NOT from the same task and they are both ddm measures
  mutate(task_task = ifelse((task_group_1 == task_group_2) & ((ddm_raw_1 == "ddm" & ddm_raw_2 == "raw") | (ddm_raw_1 == "raw" & ddm_raw_2 == "ddm")), "task-task", ifelse((task_group_1 != task_group_2) & (ddm_raw_1 == "ddm" & ddm_raw_2 == "ddm"), "ddm-ddm",NA)), 
         time="test")

#summarise by relationship between correlated variables for plotting
test_data_cor_med = test_data_cor %>%
  na.exclude() %>%
  group_by(task_task) %>%
  summarise(median_abs_cor = median(abs(value)),
            mean_abs_cor = mean(abs(value)),
            time="test")

#Get same correlation table for retest data
retest_data_cor = data.frame(cor(retest_data_std, use="complete.obs"))
retest_data_cor = retest_data_cor %>%
  mutate(dv = row.names(.)) %>%
  gather(key, value, -dv) %>%
  filter(value != 1 & duplicated(value)==FALSE)

retest_data_cor = retest_data_cor %>%
  left_join(rel_df %>% select(dv, task_group, raw_fit, rt_acc, ddm_raw), by = "dv") %>%
  rename(var_1 = dv, dv = key, task_group_1 = task_group, raw_fit_1 = raw_fit, rt_acc_1 = rt_acc, ddm_raw_1 = ddm_raw) %>%
    left_join(rel_df %>% select(dv, task_group, raw_fit, rt_acc, ddm_raw), by = "dv") %>%
  rename(var_2 = dv, task_group_2 = task_group, raw_fit_2 = raw_fit, rt_acc_2 = rt_acc, ddm_raw_2 = ddm_raw) %>%
  mutate(task_task = ifelse((task_group_1 == task_group_2) & ((ddm_raw_1 == "ddm" & ddm_raw_2 == "raw") | (ddm_raw_1 == "raw" & ddm_raw_2 == "ddm")), "task-task", ifelse((task_group_1 != task_group_2) & (ddm_raw_1 == "ddm" & ddm_raw_2 == "ddm"), "ddm-ddm",NA)),
         time="retest")

retest_data_cor_med = retest_data_cor %>%
  na.exclude() %>%
  group_by(task_task) %>%
  summarise(median_abs_cor = median(abs(value)),
            mean_abs_cor = mean(abs(value)), 
            time = "retest")

all_data_cor = rbind(test_data_cor, retest_data_cor)
all_data_cor_med = rbind(test_data_cor_med, retest_data_cor_med)

all_data_cor %>%
  na.exclude() %>%
  ggplot(aes(abs(value)))+
  geom_histogram()+
  geom_vline(data=all_data_cor_med, aes(xintercept=median_abs_cor), color="red", linetype = "dashed")+
  facet_grid(time~task_task)+
  xlab("Absolute correlation")
```

(Absolute) correlations between raw and ddm measures within a task are higher than those between ddm measures across tasks.

```{r}
all_data_cor_med
```

```{r}
summary(lmer(abs(value)~task_task+(1|var_1), tmp %>% na.exclude()))
```

Factor analysis

```{r}

```

-- Are these clusters more reliable than using either the raw or the DDM measures alone?

## Prediction
-- Do raw or DDM measures (or factor scores) predict real world outcomes better?

## To Do

Prof. Domingue recommended readings:
file:///Users/zeynepenkavi/Downloads/10.1007%252Fs11336-006-1478-z.pdf
A HIERARCHICAL FRAMEWORK FOR MODELING SPEED AND ACCURACY
ON TEST ITEMS

file:///Users/zeynepenkavi/Downloads/v66i04.pdf
Fitting Diffusion Item Response Theory Models for
Responses and Response Times Using the R
Package diffIRT