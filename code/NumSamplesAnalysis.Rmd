---
title: 'Effects of number of samples from the posterior predictive in calculating fit statistics for HDDM parameters'
output:
github_document:
toc: yes
toc_float: yes
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
input_path = '/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_DDM_Analyses/input/hddm_fitstat/num_samples_tests/'

source('/Users/zeynepenkavi/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/sem.R')

library(tidyverse)
library(lme4)

theme_set(theme_bw())
```

*Leading question:* Are HDDMs estimated from n=552 better fits to individual subject data compared to the same models fit on same subjects' data but with n=150 (i.e. only the subjects who have completed the battery twice)? To answer this we sample from the posterior predictive from each model and calculate fit statistics (e.g. regressing the predicted response times on the actual response times to calculate variance explained). By default we used 500 as the number of `samples` argument in the `post_pred_gen` function. The models using n=552 are, however, very large and for some tasks using this number can take up to a month (!) to calculate the fit statistics. Therefore we examined how the changing the input to this argument changed the fit statistics for two tasks.

*Current problem:* Does the $R^2$ increase as the `samples` argument input increases?

*Approach:* Calculate fit statistics for the choice reaction time and local global letter tasks using 10, 25, 50, 100, 250, 500 as inputs for the `samples` argument.

Read in each file, 
make column with number of samples and for task, 
concatetate to one dataframe

```{r}
fitstat_samples = data.frame()

for(f in list.files(input_path)){
  tmp = read.csv(paste0(input_path, f), row.names=NULL)
  #Extract number of samples
  num_samples = as.numeric(gsub("[^\\d]+", "", f, perl=TRUE))
  tmp$num_samples = ifelse(is.na(num_samples), 500, num_samples)
  tmp$task_name = gsub( "_refit.*$", "", f)
  fitstat_samples = rbind(fitstat_samples, tmp)
}
```

Adjusted $R^2$ of regressing predicted log RT's on true log RT's

```{r}
fitstat_samples %>%
  group_by(task_name, num_samples) %>%
  summarise(m_log_rsq_adj = mean(log_rsq_adj),
            sem_log_rsq_adj = sem(log_rsq_adj)) %>%
  ggplot(aes(as.factor(num_samples), m_log_rsq_adj, color=task_name))+
  geom_point()+
  geom_errorbar(aes(ymin = m_log_rsq_adj-sem_log_rsq_adj, ymax = m_log_rsq_adj+sem_log_rsq_adj), width=0)+
  geom_line(aes(col=task_name, group=task_name))+
  theme(legend.title=element_blank())
```

$R^2$ of regressing predicted log RT's on true log RT's

```{r}
fitstat_samples %>%
  group_by(task_name, num_samples) %>%
  summarise(m_log_rsq = mean(log_rsq),
            sem_log_rsq = sem(log_rsq)) %>%
  ggplot(aes(as.factor(num_samples), m_log_rsq, color=task_name))+
  geom_point()+
  geom_errorbar(aes(ymin = m_log_rsq-sem_log_rsq, ymax = m_log_rsq+sem_log_rsq), width=0)+
  geom_line(aes(col=task_name, group=task_name))+
  theme(legend.title=element_blank())
```

Adjusted $R^2$ of regressing predicted RT's on true RT's

```{r}
fitstat_samples %>%
  group_by(task_name, num_samples) %>%
  summarise(m_rsq_adj = mean(rsq_adj),
            sem_rsq_adj = sem(rsq_adj)) %>%
  ggplot(aes(as.factor(num_samples), m_rsq_adj, col=task_name))+
  geom_point()+
  geom_errorbar(aes(ymin = m_rsq_adj-sem_rsq_adj, ymax = m_rsq_adj+sem_rsq_adj), width=0)+
  geom_line(aes(col=task_name, group=task_name))+
  theme(legend.title=element_blank())
```

$R^2$ of regressing predicted RT's on true RT's

```{r}
fitstat_samples %>%
  group_by(task_name, num_samples) %>%
  summarise(m_rsq = mean(rsq),
            sem_rsq = sem(rsq)) %>%
  ggplot(aes(as.factor(num_samples), m_rsq, col=task_name))+
  geom_point()+
  geom_errorbar(aes(ymin = m_rsq-sem_rsq, ymax = m_rsq+sem_rsq), width = 0)+
  geom_line(aes(col=task_name, group=task_name))+
  theme(legend.title=element_blank())
```

```{r}
summary(lmer(rsq ~ task_name*num_samples +(1|subj_id), fitstat_samples))
```

*Answer:*
- Models are pretty bad fits for both tasks.  
- If you zoom in it appears like the pattern of fit dependency on the `samples` argument varies by task such that: For local global letter there doesn't seem to be a major change and for choice reaction time fits get worse and less variable with more `samples`.  
- This is not visible when you look at both together and check statistically.  
- The only effect is that local global has better fits than choice reaction time.

